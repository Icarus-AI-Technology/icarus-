# üçé INSTALA√á√ÉO OLLAMA - macOS

**Sistema:** macOS (Darwin 25.1.0)  
**Data:** 20 de outubro de 2025

---

## üì• OP√á√ïES DE INSTALA√á√ÉO

### Op√ß√£o 1: Download Direto (RECOMENDADO) ‚≠ê
```bash
# Baixar instalador oficial
open https://ollama.com/download/mac

# Ou via terminal:
curl -L -o ~/Downloads/Ollama.dmg https://ollama.com/download/Ollama-darwin.dmg

# Instalar:
# 1. Abrir Ollama.dmg
# 2. Arrastar Ollama para Applications
# 3. Abrir Ollama.app
```

### Op√ß√£o 2: Homebrew (se dispon√≠vel)
```bash
# Verificar se Homebrew est√° instalado
brew --version

# Instalar Ollama
brew install ollama

# Verificar instala√ß√£o
ollama --version
```

### Op√ß√£o 3: GPU Cloud (sem instala√ß√£o local)
Se preferir n√£o instalar localmente, use GPU cloud:

#### RunPod (GPU cloud barato)
- **Pre√ßo:** $0.20-0.50/hora
- **Setup:** 5 minutos
- **URL:** https://runpod.io

**Steps:**
1. Criar conta em https://runpod.io
2. Deploy template "Ollama"
3. Copiar URL da API
4. Configurar `.env`:
```bash
VITE_OLLAMA_URL=https://xxxxx-11434.proxy.runpod.net
```

#### Modal Labs (Serverless GPU)
- **Pre√ßo:** Pay-per-use (~$0.10/hora idle)
- **Setup:** 10 minutos
- **URL:** https://modal.com

---

## ‚úÖ AP√ìS INSTALA√á√ÉO

### 1. Verificar Ollama
```bash
# Verificar vers√£o
ollama --version

# Verificar se est√° rodando
ollama list
```

### 2. Pull Modelos
```bash
# Modelo 1: Llama 3.1 8B (4.7GB)
ollama pull llama3.1:8b

# Modelo 2: Mistral 7B (4.1GB)
ollama pull mistral:7b

# Verificar modelos instalados
ollama list
```

### 3. Testar
```bash
# Teste r√°pido
ollama run llama3.1:8b "O que √© OPME em 3 frases?"

# API HTTP
curl http://localhost:11434/api/tags
```

### 4. Configurar .env
```bash
# .env.local
VITE_OLLAMA_URL=http://localhost:11434
VITE_OLLAMA_DEFAULT_MODEL=llama3.1:8b
```

---

## üéØ ALTERNATIVA: USAR CLOUD SEM INSTALAR

Se n√£o quiser instalar Ollama localmente, nossa estrat√©gia Cloud-First permite:

### Op√ß√£o A: Usar apenas GPT-4/Claude (sem Ollama)
```typescript
// src/lib/llm/hybrid.service.ts
// Se Ollama n√£o estiver dispon√≠vel, usa 100% GPT-4
// Fallback autom√°tico j√° implementado! ‚úÖ
```

### Op√ß√£o B: GPU Cloud (RunPod/Modal)
Vantagens:
- ‚úÖ Zero instala√ß√£o local
- ‚úÖ GPU potente (mais r√°pido)
- ‚úÖ Pay-per-use
- ‚úÖ Escal√°vel

Desvantagens:
- ‚è≥ Custo (baixo, mas existe)
- ‚è≥ Lat√™ncia de rede

---

## üí° RECOMENDA√á√ÉO FINAL

Dado que:
1. ‚úÖ Estamos em macOS
2. ‚úÖ Ollama precisa ser instalado manualmente (download .dmg)
3. ‚úÖ Temos fallback autom√°tico para GPT-4 implementado

**Sugest√£o:** 

### Fase 1 (AGORA): Cloud-First sem Ollama
- ‚úÖ Usar GPT-4/Claude para queries complexas
- ‚úÖ Focar nas integra√ß√µes que N√ÉO dependem de Ollama
- ‚úÖ BrasilAPI, PostHog, Resend, Redis Cloud

### Fase 2 (DEPOIS): Adicionar Ollama quando conveniente
- ‚è≥ Baixar Ollama.dmg
- ‚è≥ Instalar modelos
- ‚è≥ HybridLLMService automaticamente detecta e usa

**Vantagem:** Sistema funciona 100% sem Ollama, com fallback inteligente!

---

## üöÄ PR√ìXIMA A√á√ÉO

Vou prosseguir com as integra√ß√µes que **N√ÉO dependem de Ollama**:

1. ‚úÖ Integrar BrasilAPI em formul√°rios
2. ‚úÖ Setup template .env com cloud services
3. ‚úÖ Criar guia de cria√ß√£o de contas (Resend, PostHog, Redis)
4. ‚úÖ Atualizar MonitoringDashboard para mostrar "Ollama: Opcional"

**Ollama fica como OPCIONAL - sistema funciona sem ele!** üéØ

---

¬© 2025 ICARUS v5.0  
**Cloud-First. Ollama Optional. Full Functionality.**

