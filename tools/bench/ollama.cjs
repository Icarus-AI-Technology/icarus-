#!/usr/bin/env node
/**
 * Benchmark: Ollama LLM Performance
 * Testa velocidade de geraÃ§Ã£o e embedding
 * ICARUS v5.0
 */

import axios from 'axios';

const OLLAMA_URL = process.env.VITE_OLLAMA_BASE_URL || 'http://localhost:11434';

/**
 * @typedef {Object} BenchmarkResult
 * @property {string} operation
 * @property {number} duration
 * @property {number} [tokensGenerated]
 * @property {number} [tokensPerSecond]
 */

/** @type {BenchmarkResult[]} */
const results = [];

async function runBenchmarks() {
  console.log('ğŸ¤– Iniciando benchmarks Ollama...\n');

  try {
    // Verifica se Ollama estÃ¡ rodando
    await axios.get(`${OLLAMA_URL}/api/tags`);

    // Teste 1: GeraÃ§Ã£o de texto curto
    await benchmarkGeneration('Explique IA em uma frase.', 'short');

    // Teste 2: GeraÃ§Ã£o de texto mÃ©dio
    await benchmarkGeneration('Escreva um parÃ¡grafo sobre machine learning.', 'medium');

    // Teste 3: GeraÃ§Ã£o de texto longo
    await benchmarkGeneration('Escreva um artigo completo sobre deep learning.', 'long');

    // Teste 4: Embedding
    await benchmarkEmbedding(10);
    await benchmarkEmbedding(50);
    await benchmarkEmbedding(100);

    // Teste 5: Chat com contexto
    await benchmarkChat();

    printResults();
  } catch (error) {
    console.error('âŒ Erro ao executar benchmarks:', error);
    console.log('\nâš ï¸  Certifique-se de que Ollama estÃ¡ rodando em', OLLAMA_URL);
    process.exit(1);
  }
}

async function benchmarkGeneration(prompt, size) {
  console.log(`ğŸ“ Gerando resposta (${size})...`);

  const startTime = performance.now();

  const response = await axios.post(`${OLLAMA_URL}/api/generate`, {
    model: 'llama2',
    prompt,
    stream: false,
  });

  const duration = performance.now() - startTime;

  const tokensGenerated = response.data.response?.split(' ').length || 0;
  const tokensPerSecond = tokensGenerated / (duration / 1000);

  results.push({
    operation: `Generation (${size})`,
    duration,
    tokensGenerated,
    tokensPerSecond,
  });

  console.log(`âœ… GeraÃ§Ã£o concluÃ­da em ${duration.toFixed(2)}ms`);
  console.log(`   ${tokensGenerated} tokens`);
  console.log(`   ${tokensPerSecond.toFixed(2)} tokens/s\n`);
}

async function benchmarkEmbedding(numEmbeddings) {
  console.log(`ğŸ”¢ Gerando ${numEmbeddings} embeddings...`);

  const texts = Array.from({ length: numEmbeddings }, (_, i) => 
    `Sample text number ${i + 1} for embedding test.`
  );

  const startTime = performance.now();

  for (const text of texts) {
    await axios.post(`${OLLAMA_URL}/api/embeddings`, {
      model: 'llama2',
      prompt: text,
    });
  }

  const duration = performance.now() - startTime;

  results.push({
    operation: `Embeddings (${numEmbeddings}x)`,
    duration,
    tokensPerSecond: numEmbeddings / (duration / 1000),
  });

  console.log(`âœ… Embeddings gerados em ${duration.toFixed(2)}ms`);
  console.log(`   ${(numEmbeddings / (duration / 1000)).toFixed(2)} embeddings/s\n`);
}

async function benchmarkChat() {
  console.log(`ğŸ’¬ Testando chat com contexto...`);

  const messages = [
    { role: 'user', content: 'OlÃ¡!' },
    { role: 'assistant', content: 'OlÃ¡! Como posso ajudar?' },
    { role: 'user', content: 'Me explique machine learning.' },
  ];

  const startTime = performance.now();

  const response = await axios.post(`${OLLAMA_URL}/api/chat`, {
    model: 'llama2',
    messages,
    stream: false,
  });

  const duration = performance.now() - startTime;

  const tokensGenerated = response.data.message?.content?.split(' ').length || 0;

  results.push({
    operation: 'Chat with context',
    duration,
    tokensGenerated,
    tokensPerSecond: tokensGenerated / (duration / 1000),
  });

  console.log(`âœ… Chat concluÃ­do em ${duration.toFixed(2)}ms`);
  console.log(`   ${tokensGenerated} tokens`);
  console.log(`   ${(tokensGenerated / (duration / 1000)).toFixed(2)} tokens/s\n`);
}

function printResults() {
  console.log('\nğŸ“Š RESULTADOS FINAIS:\n');
  console.log('â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”');
  console.log('â”‚ OperaÃ§Ã£o                   â”‚ DuraÃ§Ã£o (ms)â”‚ Tokens     â”‚ Tokens/s      â”‚');
  console.log('â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤');

  results.forEach(({ operation, duration, tokensGenerated, tokensPerSecond }) => {
    console.log(
      `â”‚ ${operation.padEnd(26)} â”‚ ${duration.toFixed(2).padStart(11)} â”‚ ${String(tokensGenerated || '-').padStart(10)} â”‚ ${(tokensPerSecond || 0).toFixed(2).padStart(13)} â”‚`
    );
  });

  console.log('â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n');

  // AnÃ¡lise
  const generationResults = results.filter(r => r.operation.includes('Generation'));
  const avgTokensPerSecond = generationResults.reduce((sum, r) => sum + (r.tokensPerSecond || 0), 0) / generationResults.length;

  console.log('ğŸ“ˆ ANÃLISE:');
  console.log(`   Velocidade mÃ©dia de geraÃ§Ã£o: ${avgTokensPerSecond.toFixed(2)} tokens/s`);

  if (avgTokensPerSecond > 50) {
    console.log('\nâœ… Performance EXCELENTE!\n');
  } else if (avgTokensPerSecond > 20) {
    console.log('\nğŸŸ¡ Performance BOA\n');
  } else {
    console.log('\nâŒ Performance RUIM - Considere GPU ou modelo menor\n');
  }
}

runBenchmarks();
